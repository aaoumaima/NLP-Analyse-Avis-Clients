# -*- coding: utf-8 -*-
"""Projet_NLP19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12xQRG_ED98OUfPe-EdWfYWEoF2Oqx9wx
"""



import os
import numpy as np
import pandas as pd
import torch

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from torch.utils.data import DataLoader
from tqdm import tqdm

import os, random, numpy as np, torch

def set_seed(seed=42):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    # mode d√©terministe
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

path = "/content/sample_data/TA_restaurants_ML_clean.csv"

df = pd.read_csv(
    path,
    engine="python",
    sep=",",              # ton fichier est g√©n√©ralement s√©par√© par virgule
    quotechar='"',
    escapechar="\\",
    on_bad_lines="skip",  # ignore les lignes corrompues
    encoding="utf-8"
)

print(" Shape:", df.shape)
print(" Colonnes:", list(df.columns))
df.head()

# V√©rifie que les colonnes existent
print(df.columns)

# On garde Review + Rating
df = df[["Review", "Rating"]].dropna()

# convertir rating en num√©rique (important)
df["Rating"] = pd.to_numeric(df["Rating"], errors="coerce")
df = df.dropna(subset=["Rating"])

def rating_to_label(r):
    if r <= 2:
        return 0   # N√©gatif
    elif r == 3:
        return 1   # Neutre
    else:
        return 2   # Positif

df["label"] = df["Rating"].apply(rating_to_label)

print("Distribution des labels:")
print(df["label"].value_counts())

df[["Review", "Rating", "label"]].head()

# Garder uniquement les labels avec au moins 2 exemples
counts = df["label"].value_counts()
valid_labels = counts[counts >= 2].index

df = df[df["label"].isin(valid_labels)]

print("Nouvelle distribution :")
print(df["label"].value_counts())

df["Review"] = df["Review"].astype(str).str.lower().str.strip()
df = df.drop_duplicates(subset=["Review"]).reset_index(drop=True)
print("After drop_duplicates:", df.shape)

texts = df["Review"].astype(str).tolist()
labels = df["label"].astype(int).tolist()

from sklearn.model_selection import train_test_split

# texts & labels
texts = df["Review"].astype(str).tolist()
labels = df["label"].astype(int).tolist()

train_texts, temp_texts, train_labels, temp_labels = train_test_split(
    texts, labels,
    test_size=0.2,
    random_state=42,
    stratify=labels
)

val_texts, test_texts, val_labels, test_labels = train_test_split(
    temp_texts, temp_labels,
    test_size=0.5,
    random_state=42,
    stratify=temp_labels
)

print("Train:", len(train_texts), "Val:", len(val_texts), "Test:", len(test_texts))

from datasets import Dataset
from transformers import AutoTokenizer

model_name = "distilbert-base-uncased"   # simple et rapide pour examen
tokenizer = AutoTokenizer.from_pretrained(model_name)

train_ds = Dataset.from_dict({"text": train_texts, "label": train_labels})
val_ds   = Dataset.from_dict({"text": val_texts,   "label": val_labels})
test_ds  = Dataset.from_dict({"text": test_texts,  "label": test_labels})

def tokenize_batch(batch):
    return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=64)

train_ds = train_ds.map(tokenize_batch, batched=True)
val_ds   = val_ds.map(tokenize_batch, batched=True)
test_ds  = test_ds.map(tokenize_batch, batched=True)

cols = ["input_ids", "attention_mask", "label"]
train_ds.set_format("torch", columns=cols)
val_ds.set_format("torch", columns=cols)
test_ds.set_format("torch", columns=cols)

print("Datasets OK")

from torch.utils.data import DataLoader
import torch

g = torch.Generator().manual_seed(42)

train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, generator=g)
val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False)
test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False)

print("Loaders OK")

from transformers import AutoModelForSequenceClassification
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model_name = "distilbert-base-uncased"   # rapide et fiable pour examen
num_labels = 3   # Neg / Neu / Pos

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels
)

model.to(device)
print("Model OK")

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)
epochs = 1

def tokenize_batch(batch):
    return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=64)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import torch

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        preds = torch.argmax(outputs.logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

print("Accuracy =", round(accuracy_score(all_labels, all_preds)*100, 2), "%")

print("\nClassification report:\n",
      classification_report(
          all_labels,
          all_preds,
          target_names=["Negatif", "Neutre", "Positif"]
      ))

print("\nConfusion matrix:\n", confusion_matrix(all_labels, all_preds))

from transformers import get_linear_schedule_with_warmup
from sklearn.metrics import accuracy_score, f1_score
from tqdm import tqdm
import torch

# Optimizer + scheduler
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)

epochs = 2
total_steps = len(train_loader) * epochs
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=int(0.1 * total_steps),
    num_training_steps=total_steps
)

# Eval function
def eval_metrics(model, loader):
    model.eval()
    all_preds, all_labels = [], []

    with torch.no_grad():
        for batch in loader:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            preds = torch.argmax(outputs.logits, dim=1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    acc = accuracy_score(all_labels, all_preds)
    f1  = f1_score(all_labels, all_preds, average="weighted")
    return acc, f1

# Train loop + save best
best_val_acc = 0.0

for epoch in range(epochs):
    model.train()
    total_loss = 0.0

    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
    for batch in pbar:
        optimizer.zero_grad()

        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        scheduler.step()

        total_loss += loss.item()
        pbar.set_postfix(loss=float(loss.item()))

    train_loss = total_loss / len(train_loader)

    val_acc, val_f1 = eval_metrics(model, val_loader)

    # ‚úÖ save best model
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), "best_model.pt")

    print(f"\nEpoch {epoch+1} done | Train Loss: {train_loss:.4f}")
    print(f"VAL Accuracy: {val_acc*100:.2f}% | VAL F1: {val_f1*100:.2f}%")
    print(f"Best VAL Accuracy so far: {best_val_acc*100:.2f}%\n")

model.load_state_dict(torch.load("best_model.pt", map_location=device))
model.eval()
print(" Best model loaded:", best_val_acc)

test_acc, test_f1 = eval_metrics(model, test_loader)
print("\n TEST Accuracy:", round(test_acc*100,2), "%")
print(" TEST F1:", round(test_f1*100,2), "%")

# rapport + confusion matrix
model.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

print("\nüìå Classification report:")
print(classification_report(all_labels, all_preds, target_names=["Negatif", "Positif"]))

cm = confusion_matrix(all_labels, all_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Negatif","Positif"])
disp.plot()
plt.title("Matrice de confusion (TEST)")
plt.show()

SAVE_DIR = "/content/drive/MyDrive/bert_sentiment_exam"
model.save_pretrained(SAVE_DIR)
tokenizer.save_pretrained(SAVE_DIR)

from google.colab import drive
drive.mount("/content/drive")

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

MODEL_PATH = "/content/drive/MyDrive/bert_sentiment_exam"  # ‚úÖ mets ton dossier exact

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)

model.to(device)
model.eval()

print(" Model loaded from:", MODEL_PATH)

label_map = {0: "Negatif", 1: "Positif"}

def predict(text):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=128
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1)[0]
        pred = torch.argmax(probs).item()
        conf = float(probs[pred].item()) * 100

    return label_map[pred], round(conf, 2)

def satisfaction_and_loyalty(sentiment, conf):
    # r√®gle simple (tu peux changer)
    if sentiment == "Positif" and conf >= 60:
        return "Satisfait ", "Va probablement revenir "
    if sentiment == "Negatif" and conf >= 60:
        return "Non satisfait ", "Risque de ne pas revenir "
    return "Moyen ", "Incertain"

def chatbot_review(text):
    sentiment, conf = predict(text)
    sat, loyalty = satisfaction_and_loyalty(sentiment, conf)

    return {
        "Avis": text,
        "Sentiment": sentiment,
        "Confiance (%)": conf,
        "Satisfaction": sat,
        "Fidelite": loyalty
    }

# test rapide
chatbot_review("The food was amazing but the service was slow")

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# si tu as d√©j√† model + tokenizer
model.save_pretrained("model")
tokenizer.save_pretrained("model")

print(" model/ saved")

model.save_pretrained("model")
tokenizer.save_pretrained("model")



while True:
    text = input("Ecris un avis (ou 'exit'): ")
    if text.lower() == "exit":
        break

    result = chatbot_review(text)
    print("\n--- R√©sultat ---")
    for k, v in result.items():
        print(f"{k}: {v}")
    print()